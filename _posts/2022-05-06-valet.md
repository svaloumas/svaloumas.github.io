---
layout: single
title: "Valet"
data: 2022-05-06
---

# A job queuing and async task running service packed in a Go module

![queuing-mechanism](../assets/images/queuing-mech.jpg)

_A few months ago, I stumbled upon this great [post](https://medium.com/@matiasvarela/hexagonal-architecture-in-go-cfd4e436faa3) by @matiasvarela, in which the author makes an introduction to the hexagonal architecture by walking us through an example project. That inspired me to build this new project by practicing the specific design pattern._

## What is Valet?

The idea is not new. A job queuing service and an async task runner. A daemon process that exposes an API and is responsible for the execution of “heavy” tasks, asynchronously and concurrently. Something that lives somewhere between other services and has just one responsibility: to do the heavy work and store the results, if any.

So what would be a suitable task? How could a task be scheduled for execution? How could we pass parameters to each task and in what way could we profit from the built-in support of the language for concurrency to make all this more efficient? Let’s answer those questions by stepping into the implementation details of the project.

You can find the project on my GitHub [repo](https://github.com/svaloumas/valet).

It goes without saying that every approach presented here is just one of the possible solutions and any feedback would be greatly appreciated.

## Callbacks

A task is a user-defined function that is run as a callback by the service sometime in the future or just instantly. Valet handles every possible callback in the same way as follows:

Every task should implement a fixed type. The signature `func(…interface{}) (interface{}, error)` is used as the task’s type.

```go
// TaskFunc is the type of the task callback.
type TaskFunc func(...interface{}) (interface{}, error)

// TaskRepository is the in memory task repository.
type TaskRepository map[string]TaskFunc
```

> TaskFunc is the type that tasks should implement.

You can pass arguments to the functions through API endpoints. Those arguments will always land in `args[0]` so they can be accessed from inside the functions, and their type will be `map[string]interface{}`. As we’re going to see later, multiple tasks can be sequenced to run one after another. In this case, `args[1]` will hold the results of the previous task so that the current one can use them along with its own parameters. The return values of a task are always an `interface{}` with any metadata we might need to store as a task result and an `error`.

You can also define custom structs and decode the `map[string]interface{}` typed parameters, which are given in the request payload, into your own data structures.

The two functions below decode the arguments into custom structs. Both of them use internally @Mitchell Hashimoto’s [mapstructure](https://github.com/mitchellh/mapstructure).

```go
// DecodeTaskParams uses https://github.com/mitchellh/mapstructure
// to decode task params to a pointer of map or struct.
func DecodeTaskParams(args []interface{}, params interface{}) {
	mapstructure.Decode(args[0], params)
}

// DecodeTaskParams uses https://github.com/mitchellh/mapstructure
// to safely decode previous job's results metadata to a pointer of map or struct.
func DecodePreviousJobResults(args []interface{}, results interface{}) {
	if len(args) == 2 {
		mapstructure.Decode(args[1], results)
	}
}
```

> Helper functions to use in your task functions.

Let’s put it all together. Here’s an example of a user-defined task implementing the `TaskFunc` type, and a struct to decode the task parameters into.

```go
package task

import (
	"github.com/svaloumas/valet"
)

// DummyParams is an example of a task params structure.
type DummyParams struct {
	URL string `json:"url,omitempty"`
}

// DummyTask is a dummy task callback.
func DummyTask(args ...interface{}) (interface{}, error) {
	dummyParams := &DummyParams{}
	var resultsMetadata string
	valet.DecodeTaskParams(args, dummyParams)
	valet.DecodePreviousJobResults(args, &resultsMetadata)

	metadata := downloadContent(dummyParams.URL, resultsMetadata)
	return metadata, nil
}

func downloadContent(URL, bucket string) string {
	return "some metadata"
}
```

> A dummy task callback example.

Tasks are registered at service initialization and are stored in a map in memory so that their function values can be invoked using a unique string identifier.

```go
func main() {
	v := valet.New("config.yaml")
	v.RegisterTask("dummytask", task.DummyTask)
        v.RegisterTask("anothertask", task.AnotherTask)
        ...
}
```

> Task registration during service initialization.

The service uses a simple lookup mechanism at runtime, to correlate each task name with its corresponding function value.

```go
// GetTaskFunc returns the TaskFunc for a specified name if that exists in the task repository.
func (repo TaskRepository) GetTaskFunc(name string) (TaskFunc, error) {
	task, ok := repo[name]
	if !ok {
		return nil, fmt.Errorf("task with name: %s is not registered", name)
	}
	return task, nil
}
```

> In-memory task lookup.

Now that we have the tasks in memory, we can create new jobs and pipelines passing our parameters to the functions through an API call, by adding them to the request payload.

```json
{
    "name": "a job",
    "description": "what this job job is all about, but briefly",
    "task_name": "dummytask",
    "task_params": {
        "url": "www.some-fake-url.com"
    },
    "run_at": "2022-06-06T15:04:05.999",
    "timeout": 10
}
```

> Sample Job creation request payload.

So, what’s a job and what’s a pipeline?

## Hexagonal architecture

If you haven’t already, spend some minutes reading Matias’ post about this design pattern (you can find the link below).

### Domain

At the core of the project resides the domain. This is where the data structures related to the business logic live. Valet’s main domain entity is the _job_.

A job simply encapsulates all the metadata related to the task that will run. Information like the timeout interval that we set for a specific job before being marked as failed, the status of the job, whether it’s pending, in-progress, or completed as well as timestamps of when it was created, started or completed is part of a job.

Another main entity is the _pipeline_, which represents a sequence of jobs that need to be run one after another in a specified order. When a pipeline is executed, every job can optionally use the results of the previous completed job.

### Ports (Interfaces)

Valet requires two “external” systems to operate properly. A queue, to implement its job queuing mechanism and a storage system to persist the job’s and/or the pipeline’s metadata and its results.

Both the queue and the storage systems are driven actors in the terminology of the design pattern. Defined are two ports to enable the interaction with the actors, _JobQueue_ & _Storage_.

For driver actors like a CLI tool or any other app, to create new or manage existing jobs and pipelines, driver actor interfaces are also defined, along with their concrete implementations, the so-called services.

### Adapters

Currently, the project provides three concrete implementations of the JobQueue interface. A simple yet lightweight in-memory FIFO queue in the form of a job channel,

```go
package jobqueue

import (
	"github.com/svaloumas/valet/internal/core/domain"
	"github.com/svaloumas/valet/internal/core/port"
	"github.com/svaloumas/valet/pkg/apperrors"
)

var _ port.JobQueue = &fifoqueue{}

type fifoqueue struct {
	jobs     chan *domain.Job
	capacity int
}

// NewFIFOQueue creates and returns a new fifoqueue instance.
func NewFIFOQueue(capacity int) *fifoqueue {
	return &fifoqueue{
		jobs:     make(chan *domain.Job, capacity),
		capacity: capacity,
	}
}

// Push adds a job to the queue.
func (q *fifoqueue) Push(j *domain.Job) error {
	select {
	case q.jobs <- j:
		return nil
	default:
		return &apperrors.FullQueueErr{}
	}
}

// Pop removes and returns the head job from the queue.
func (q *fifoqueue) Pop() *domain.Job {
	select {
	case j := <-q.jobs:
		return j
	default:
		return nil
	}
}
```

> Simple in-memory FIFO queue implementation.

a RabbitMQ client, to connect the service with an existing message broker, and a Redis client that leverages `LPUSH` and `RPOP` Redis commands, to use the NoSQL storage as a job queue.

As for the storage interface, four concrete implementations are provided, making the service a plug-and-play component for many environments. An in-memory key-value storage for when no external storage is available or for testing purposes. MySQL and PostgreSQL are supported from the land of relational storage systems. Finally, Redis as a NoSQL representative can also be utilized for persistence as there’s an available adapter for it.

Regarding driver actor adapters, provided are handler implementations that enable communication both over HTTP and gRPC.

### Dependency injection

Another major win of having the project layout structured following this approach is that we can leverage dependency injection when initializing the different internal components. That also involves the use of the factory pattern.

```go
jobQueue := factory.JobQueueFactory(cfg.JobQueue, cfg.LoggingFormat)
v.logger.Infof("initialized [%s] as a job queue", cfg.JobQueue.Option)

storage := factory.StorageFactory(cfg.Storage)
v.logger.Infof("initialized [%s] as a storage", cfg.Storage.Option)
```

> Use of the factory pattern initializing the adapters.

```go
pipelineService := pipelinesrv.New(storage, taskrepo, uuidgen.New(), rtime.New())
jobService := jobsrv.New(storage, taskrepo, uuidgen.New(), rtime.New())
resultService := resultsrv.New(storage)
```

> Dependency injection to initialize the services.

## Concurrency

The internal component that runs the tasks is a basic worker pool implementation. The number of the available workers is configurable as well as the worker pool internal queue.

```go
// Start starts the worker pool.
func (srv *workservice) Start() {
	for i := 0; i < srv.workers; i++ {
		srv.wg.Add(1)
		go srv.startWorker(i, srv.queue, &srv.wg)
	}
	srv.logger.Infof("set up %d workers with a queue of capacity %d", srv.workers, srv.queue_capacity)
}
```

> Initialize a configurable number of workers.

Each worker listens to this internal queue for work to be done, consumes the messages, and performs the work.

```go
for work := range queue {
	srv.logger.Infof("%s executing %s...", logPrefix, work.Type)
	if err := srv.Exec(context.Background(), work); err != nil {
		srv.logger.Errorf("could not update job status: %s", err)
	}
	srv.logger.Infof("%s %s finished!", logPrefix, work.Type)
}
```

> Worker listen to the worker pool queue for work.

In case of a pipeline, all the tasks are executed by the same worker, as they have to wait for each task to finish, before proceeding to the next one.

## Running the tasks

Handling a user-defined function can be tricky. The best-case scenario is that the function runs successfully, or even produces an error value, which is expected and its message will persist along with any results metadata. But a function can easily introduce a deadlock, resulting in eternal waiting and of course resource leaks. It can also happily panic.

A panic is an exception in Go. Nevertheless, it’s able enough to propagate its error stack trace up to the main thread of execution and damage the whole process. We certainly cannot afford something like that for a service that needs to run at least hundreds of those functions! We have to handle panic as if it were an ordinary `error`. We can do that by leveraging the built-in `recover()` function. An anonymous func is deferred to check if a panic occurred, extract the panic message, and send it over a channel as a job result which will end up in our persistence layer.

```go
func (srv *workservice) work(
	job *domain.Job,
	jobResultChan chan domain.JobResult,
	previousJobResultsMetadata interface{}) {

	go func() {
		defer func() {
			if p := recover(); p != nil {
				result := domain.JobResult{
					JobID:    job.ID,
					Metadata: nil,
					Error:    fmt.Errorf("%v", p).Error(),
				}
				jobResultChan <- result
				close(jobResultChan)
			}
		}()
		var errMsg string

		// Should be already validated.
		taskFunc, _ := srv.taskrepo.GetTaskFunc(job.TaskName)

		params := []interface{}{
			job.TaskParams,
		}
		if job.UsePreviousResults && previousJobResultsMetadata != nil {
			params = append(params, previousJobResultsMetadata)
		}
		// Perform the actual work.
		resultMetadata, jobErr := taskFunc(params...)
		if jobErr != nil {
			errMsg = jobErr.Error()
		}

		result := domain.JobResult{
			JobID:    job.ID,
			Metadata: resultMetadata,
			Error:    errMsg,
		}
		jobResultChan <- result
		close(jobResultChan)
	}()
}
```

> The go-routine responsible for the task execution.

As for the potential deadlocks, there’s not much we can do, at least with this simple and not-so-sophisticated approach. Instead of trying to stop a running go-routine under a deadlock condition, we can set a timeout interval for our task. That way, we still lose some resources in case of a deadlock, but we should be able to continue serving other tasks, instead of waiting forever for the problematic one to end. After all, it’s up to the user to test the behavior of their own functions before registering them as tasks in the service.

```go
ctx, cancel := context.WithTimeout(ctx, timeout)
defer cancel()

jobResultChan := make(chan domain.JobResult, 1)

srv.work(w.Job, jobResultChan, nil)

var jobResult domain.JobResult
select {
case <-ctx.Done():
	failedAt := srv.time.Now()
	w.Job.MarkFailed(&failedAt, ctx.Err().Error())

	jobResult = domain.JobResult{
		JobID:    w.Job.ID,
		Metadata: nil,
		Error:    ctx.Err().Error(),
	}
case jobResult = <-jobResultChan:
	if jobResult.Error != "" {
		failedAt := srv.time.Now()
		w.Job.MarkFailed(&failedAt, jobResult.Error)
	} else {
		completedAt := srv.time.Now()
		w.Job.MarkCompleted(&completedAt)
	}
}
if err := srv.storage.UpdateJob(w.Job.ID, w.Job); err != nil {
	return err
}
w.Result <- jobResult
```

> Waiting for a job result or a timeout, whichever comes first.

## Go module

The initial idea was to offer valet as a `cmd` so we could build and spin out a Docker container with the service and start using it. With this approach, we would have to download the source code of the project and place our task functions inside a specific directory of the project. From there, we would either have some script to parse our source files and register the tasks by modifying some part of the _valet_ source code before building the executable, or we would have to do it on our own. That would be both tedious and error-prone. The worst part is that we’d have to maintain those functions in two places, under some repository of ours and the fork of the valet repository.

To make things easier for everyone, the service is provided as a Go module instead. You can maintain your task functions under your repository and build your own copy of the executable just by implementing a simple `main` function, like the one shown below.

```go
// cmd/valetd/main.go
package main

import (
	"github.com/svaloumas/valet"
	"github.com/svaloumas/valet/task"
)

func main() {
	v := valet.New("config.yaml")
	v.RegisterTask("dummytask", task.DummyTask)
	v.Run()
}
```

> An example of a main function, required to build your own valet cmd.

Find the complete installation guide in the corresponding [README](https://github.com/svaloumas/valet#installation) section.

## References

* [matiasvarela/hexagonal-architecture-in-go](https://medium.com/@matiasvarela/hexagonal-architecture-in-go-cfd4e436faa3)
* [ddd_and_hexagonal](https://vaadin.com/learn/tutorials/ddd/ddd_and_hexagonal)
* [mapstructure](https://github.com/mitchellh/mapstructure)
